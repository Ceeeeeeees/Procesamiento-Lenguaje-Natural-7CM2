DOI,TokensTitulo,TokensAbstract,Autores,Fecha,Seccion
10.48550./arXiv.2503.11571,rasa replace say training free framework audio drive universal portrait video editing,portrait video editing focus modify specific attribute portrait video guide audio video stream previous method typically concentrate lip region reenactment require train specialized model to extract keypoint motion transfer new identity paper introduce training free universal portrait video editing framework provide versatile adaptable editing strategy framework support portrait appearance editing condition change first reference frame as well lip editing condition varied speech combination be base unified animation control uac mechanism source inversion latent to edit entire portrait include visual drive shape control audio drive speaking control inter frame temporal control furthermore method can be adapt different scenario adjust initial reference frame enable detailed editing portrait video specific head rotation facial expression comprehensive approach ensure holistic flexible solution portrait video editing experimental result show model can achieve more accurate synchronized lip movement lip edit task as well more flexible motion transfer appearance edit task demo be available https url,"Tianrui Pan, Lin Liu, Jie Liu, Xiaopeng Zhang, Jie Tang, Gangshan Wu, Qi Tian",14/03/2025,Vision&Patrones
10.48550./arXiv.2503.11647,recammaster camera control generative rendering single video,camera control have be actively study text image condition video generation task however alter camera trajectory give video remain under explore importance field video creation be non trivial due extra constraint maintain multiple frame appearance dynamic synchronization to address present recammaster camera control generative video re render framework reproduce dynamic scene input video novel camera trajectory core innovation lie harness generative capability pre train text video model simple yet powerful video conditioning mechanism capability often overlook current research to overcome scarcity qualified training datum construct comprehensive multi camera synchronized video dataset use unreal engine be carefully curate to follow real world filming characteristic cover diverse scene camera movement help model generalize wild video lastly far improve robustness to diverse input meticulously design training strategy extensive experiment tell method substantially outperform exist state art approach strong baseline method also find promising application video stabilization super resolution outpainte project page https url,"Jianhong Bai, Menghan Xia, Xiao Fu, Xintao Wang, Lianrui Mu, Jinwen Cao, Zuozhu Liu, Haoji Hu, Xiang Bai, Pengfei Wan, Di Zhang",14/03/2025,Vision&Patrones
10.48550./arXiv.2503.11633,see see glass real synthetic datum multi layer depth estimation,transparent object be common daily life understand multi layer depth information perceive transparent surface object be crucial real world application interact transparent material paper introduce layereddepth first dataset multi layer depth annotation include real world benchmark synthetic data generator to support task multi layer depth estimation real world benchmark consist image diverse scene evaluate state art depth estimation method reveal struggle transparent object synthetic datum generator be fully procedural capable provide training datum task unlimited variety object scene composition use generator create synthetic dataset image baseline model train solely synthetic dataset produce good cross domain multi layer depth estimation fine tune state art single layer depth model substantially improve performance transparent object quadruplet accuracy benchmark increase image validation annotation be available https url,"Hongyu Wen, Yiming Zuo, Venkat Subramanian, Patrick Chen, Jia Deng",14/03/2025,Vision&Patrones
10.48550./arXiv.2503.11579,vamba understand hour long video hybrid mamba transformer,state art transformer base large multimodal model lmms struggle to handle hour long video input quadratic complexity causal self attention operation lead high computational cost training inference exist token compression base method reduce number video token often incur information loss remain inefficient extremely long sequence paper explore orthogonal direction to build hybrid mamba transformer model vamba employ block encode video token linear complexity token reduction vamba can encode more frame single gpu transformer base model can only encode frame long video input vamba achieve least reduction gpu memory usage training inference nearly double speed training step compare transformer base lmms experimental result demonstrate vamba improve accuracy challenging hour long video understand benchmark lvbench prior efficient video lmms maintain strong performance broad spectrum long short video understanding task,"Weiming Ren, Wentao Ma, Huan Yang, Cong Wei, Ge Zhang, Wenhu Chen",14/03/2025,Vision&Patrones
10.48550./arXiv.2503.11571,rasa replace say training free framework audio drive universal portrait video editing,portrait video editing focus modify specific attribute portrait video guide audio video stream previous method typically concentrate lip region reenactment require train specialized model to extract keypoint motion transfer new identity paper introduce training free universal portrait video editing framework provide versatile adaptable editing strategy framework support portrait appearance editing condition change first reference frame as well lip editing condition varied speech combination be base unified animation control uac mechanism source inversion latent to edit entire portrait include visual drive shape control audio drive speaking control inter frame temporal control furthermore method can be adapt different scenario adjust initial reference frame enable detailed editing portrait video specific head rotation facial expression comprehensive approach ensure holistic flexible solution portrait video editing experimental result show model can achieve more accurate synchronized lip movement lip edit task as well more flexible motion transfer appearance edit task demo be available https url,"Tianrui Pan, Lin Liu, Jie Liu, Xiaopeng Zhang, Jie Tang, Gangshan Wu, Qi Tian",14/03/2025,Vision&Patrones
